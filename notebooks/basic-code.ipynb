{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitpytorch110condab050f83f33474bd49b114c64e9b47d9e",
   "display_name": "Python 3.8.5 64-bit ('pytorch-110': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "## Introduction\n",
    "  \n",
    "This tutorial explains a method to predict a sentiment of a twitt, in which it will be used RRNs as Deep Learning models. In general, the use of data in Social Networks is being exploited by the industry. For example, the analysis of sentiment of twits can be useful for a company that wants to analyse its new products. However, it is impossible that humans analyse each twit, therefore training deep learning models to predict is a way to scale the analysis.\n",
    "\n",
    "The data used in this tutorial comes from an investigation made from Stanford’s researchers. They have collected 1.4 millions of twits and classify them as positive or negative emotions based on emoticons written in the same twit, simulating Facebook. The table below contains the emotion category of the emoticon used. The data was downloaded by using the HugginFace API.\n",
    "  \n",
    "The structure of the tutorial is divided in three parts, and it will be explained: first, the steps to clean twits in order to have a dataset to do the predictions; second, the description and use of GloVe as a Word Embedding model; third, the use of LSTM in predicting sentiments. The idea of using RRNs is that sequence of words have information that can be used to predict the sentiment."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2728fcd200e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import random\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "source": [
    "## Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 4.12kB [00:00, 325kB/s]                    \n",
      "Downloading: 1.59kB [00:00, 362kB/s]                  \n",
      "Downloading and preparing dataset sentiment140/sentiment140 (download: 77.59 MiB, generated: 215.36 MiB, post-processed: Unknown size, total: 292.95 MiB) to /home/nftd/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0/9fe1c0ce3319c47cc65ff7e49aac6c34d9c050ab1432988c104b3b275e360f3f...\n",
      "Downloading: 100%|██████████| 81.4M/81.4M [01:57<00:00, 691kB/s] \n",
      "                                Dataset sentiment140 downloaded and prepared to /home/nftd/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0/9fe1c0ce3319c47cc65ff7e49aac6c34d9c050ab1432988c104b3b275e360f3f. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sentiment140\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_twitts(data,twitt_n= 10000):\n",
    "    \"\"\"Read twitts.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : DatasetDict\n",
    "        Dataset loaded from hugginface\n",
    "    twitt_n : int\n",
    "        Number of twitts to use. This is to handle the use of memory\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    data_train : list\n",
    "        Train data\n",
    "    labels_train : list\n",
    "        Train label\n",
    "    data_test : list\n",
    "        Test data\n",
    "    labels_test : list\n",
    "        Test label\n",
    "    \"\"\"\n",
    "    set_ = 'train'\n",
    "    randomlist = random.sample(range(0, 1600000), twitt_n)\n",
    "    trainrandomlist = random.sample(randomlist,int(len(randomlist)*0.8))\n",
    "    testrandomlist = []\n",
    "    for index in randomlist:\n",
    "        if index not in trainrandomlist:\n",
    "            testrandomlist.append(index)\n",
    "    #training set\n",
    "    data_train, labels_train = [],[]\n",
    "    for i in trainrandomlist:\n",
    "        data_train.append(data[set_][i]['text'])\n",
    "        labels_train.append(data[set_][i]['sentiment'])\n",
    "    #test set\n",
    "    data_test, labels_test = [],[]\n",
    "    for i in testrandomlist:\n",
    "        data_test.append(data[set_][i]['text'])\n",
    "        labels_test.append(data[set_][i]['sentiment'])\n",
    "    return data_train,labels_train,data_test,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = read_twitts(dataset,16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_info(data,label, k = 10):\n",
    "    print('# trainings:', len(data))\n",
    "    for x, y in zip(label[0:k], data[0:k]):\n",
    "        print('label:', x, 'review:', y)\n",
    "    for x, y in zip(label[-k:-1], data[-k:-1]):\n",
    "        print('label:', x, 'review:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# trainings: 12800\nlabel: 0 review: @alexisamore I  slacked off with that for like 2 weeks now... been too busy \nlabel: 4 review: eating Mother's frosted oatmeal cookies &amp;&amp; milk. \nlabel: 4 review: Testing that my laptop works from home  Yes it does \nlabel: 4 review: Tom Hanks is a Trek man!! \nlabel: 4 review: @michael_sargent Hmmm, not too late for me to change my ASB's name either.  \nlabel: 4 review: Follow my best freind @Sidrraah she is beautiful lol \nlabel: 0 review: Ugh caaaake \nlabel: 4 review: Is very thankful for her amazing friend @m3r3h because she's helping him choreograph at 630am. \nlabel: 0 review: My email system was hacked and lots of my contacts have received email from an Indian web community \nlabel: 0 review: @svanwessem Thant stinks, sorry to hear that about the stolen money \nlabel: 4 review: Alex makes my life so much better. \nlabel: 0 review: @stevexmetal i love you \nlabel: 4 review: Church at St. Pats, NBC tour, planet hollywood dinner, and TIMES SQUARE!!!! \nlabel: 4 review: @NeekyT    danika you are.   \nlabel: 0 review: Thinkin about milley \nlabel: 4 review: ok. i'll finish my paper and watch gossip girl \nlabel: 4 review: @jasonkoen Come over and you'll have one. lol Well, my darling potato salad maker, have a good day nevertheless.  xxx\nlabel: 4 review: @NickyTvf yeah, you gotta give back \nlabel: 0 review: i'm losing my clothes, first my black pants, and now i can't find my dickies jacket \n"
     ]
    }
   ],
   "source": [
    "data_info(train_data,train_label)"
   ]
  },
  {
   "source": [
    "## Data cleaning\n",
    "In this section it will cover a step-by-step guide on how text can be cleaned, the objective is to transform the data into numbers so the models can be trained. It is necessary to understand the twit structure before designing a pipeline, below it is showed an example of a twit which was classified as negative:\n",
    "  \n",
    "> @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer. You shoulda got David Carr of Third Day to do it. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer. You shoulda got David Carr of Third Day to do it.\""
   ]
  },
  {
   "source": [
    "The steps to transform the data are the following:\n",
    "### Remove users\n",
    "This is a simple step; the objective is to identify words which contains an @ at their left side."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " http://twitpic.com/2y1zl - Awww, that's a bummer. You shoulda got David Carr of Third Day to do it.\n"
     ]
    }
   ],
   "source": [
    "def remove_user(txt):\n",
    "    return re.sub('@[^\\s]+','',txt)\n",
    "twit = remove_user(twit)\n",
    "print(twit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# trainings: 12800\nlabel: 0 review:  I  slacked off with that for like 2 weeks now... been too busy \nlabel: 4 review: eating Mother's frosted oatmeal cookies &amp;&amp; milk. \nlabel: 4 review: Testing that my laptop works from home  Yes it does \nlabel: 4 review: Tom Hanks is a Trek man!! \nlabel: 4 review:  Hmmm, not too late for me to change my ASB's name either.  \nlabel: 4 review: Follow my best freind  she is beautiful lol \nlabel: 0 review: Ugh caaaake \nlabel: 4 review: Is very thankful for her amazing friend  because she's helping him choreograph at 630am. \nlabel: 0 review: My email system was hacked and lots of my contacts have received email from an Indian web community \nlabel: 0 review:  Thant stinks, sorry to hear that about the stolen money \nlabel: 4 review: Alex makes my life so much better. \nlabel: 0 review:  i love you \nlabel: 4 review: Church at St. Pats, NBC tour, planet hollywood dinner, and TIMES SQUARE!!!! \nlabel: 4 review:     danika you are.   \nlabel: 0 review: Thinkin about milley \nlabel: 4 review: ok. i'll finish my paper and watch gossip girl \nlabel: 4 review:  Come over and you'll have one. lol Well, my darling potato salad maker, have a good day nevertheless.  xxx\nlabel: 4 review:  yeah, you gotta give back \nlabel: 0 review: i'm losing my clothes, first my black pants, and now i can't find my dickies jacket \n"
     ]
    }
   ],
   "source": [
    "train_data_clean = [remove_user(i) for i in train_data]\n",
    "data_info(train_data_clean,train_label)"
   ]
  },
  {
   "source": [
    "### Remove URL\n",
    "Although hyperlink can have important information, it will take out from the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "switchfoot Awww thats a bummer You shoulda got David Carr of Third Day to do it\n"
     ]
    }
   ],
   "source": [
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "twit = remove_url(twit)\n",
    "print(twit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# trainings: 12800\nlabel: 0 review: I slacked off with that for like 2 weeks now been too busy\nlabel: 4 review: eating Mothers frosted oatmeal cookies ampamp milk\nlabel: 4 review: Testing that my laptop works from home Yes it does\nlabel: 4 review: Tom Hanks is a Trek man\nlabel: 4 review: Hmmm not too late for me to change my ASBs name either\nlabel: 4 review: Follow my best freind she is beautiful lol\nlabel: 0 review: Ugh caaaake\nlabel: 4 review: Is very thankful for her amazing friend because shes helping him choreograph at 630am\nlabel: 0 review: My email system was hacked and lots of my contacts have received email from an Indian web community\nlabel: 0 review: Thant stinks sorry to hear that about the stolen money\nlabel: 4 review: Alex makes my life so much better\nlabel: 0 review: i love you\nlabel: 4 review: Church at St Pats NBC tour planet hollywood dinner and TIMES SQUARE\nlabel: 4 review: danika you are\nlabel: 0 review: Thinkin about milley\nlabel: 4 review: ok ill finish my paper and watch gossip girl\nlabel: 4 review: Come over and youll have one lol Well my darling potato salad maker have a good day nevertheless xxx\nlabel: 4 review: yeah you gotta give back\nlabel: 0 review: im losing my clothes first my black pants and now i cant find my dickies jacket\n"
     ]
    }
   ],
   "source": [
    "train_data_clean = [remove_url(i) for i in train_data_clean]\n",
    "data_info(train_data_clean,train_label)"
   ]
  },
  {
   "source": [
    "### Tokenize\n",
    "Tokenize means that the words in the text will be split by a delimiter, normalize by a function and put the words in a list. In our case, it will split the words by spaces and it will transform to undercase all of the words. The result are tokens and an example it is showed below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-be49fabcd9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"basic_english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}